{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33bb9c72-2238-417a-8bf5-9683c3c2eda6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cfba43017f6a23d4026f663b5cc92a40",
     "grade": false,
     "grade_id": "cell-a5f55ce3ca858b71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# [1.2: Evaluation] — How do we evaluate recommender systems?\n",
    "\n",
    "In the previous notebook, you implemented a kNN-based collaborative filtering system. But this raises an important question: **how well does it actually work?**\n",
    "To answer that, we need systematic ways to evaluate the quality of its predictions.\n",
    "\n",
    "A kNN recommender does two things:\n",
    "\n",
    "1. **Predict ratings** for items a user has not seen yet (a regression task).\n",
    "2. **Convert those predicted ratings into recommendations** (a classification task).\n",
    "\n",
    "This means we can evaluate the algorithm in two complementary ways:\n",
    "\n",
    "* **As a regression model**:\n",
    "  Here we assess how close the predicted ratings are to the actual ratings.\n",
    "  A common metric for this is **Mean Squared Error (MSE)**.\n",
    "\n",
    "* **As a classification model**:\n",
    "  Once we convert predictions into “recommended” vs. “not recommended,” we can evaluate how often the recommendations are correct.\n",
    "  Two widely used metrics for this are **precision** and **recall**.\n",
    "\n",
    "Each perspective captures different aspects of performance. A model with low MSE may still make poor recommendations, and a model with high precision may fail to recommend enough relevant items.\n",
    "\n",
    "In this notebook, we will explore both evaluation approaches and see how they complement each other.\n",
    "\n",
    "Keep in mind that we are working with an **extremely small dataset** (the same one used in the previous notebook). The evaluations we perform here do **not** reflect how the algorithm would behave in real-world scenarios. The goal of this notebook is primarily to help you become familiar with different evaluation methods.\n",
    "\n",
    "In the next not_ebook, we will apply these techniques to a much larger and more realistic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f7b76c-5ee0-48a2-8b28-0b5b8fbfb172",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "632880cd799c4fadc5e47e223b976c25",
     "grade": false,
     "grade_id": "cell-b6bff95042fd70ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "### Libraries\n",
    "\n",
    "Let's start by loading the libraries we'll need by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a04fda0-bdf6-4e7d-aea4-05cfb7e4e632",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "410d4465c97b00689196db028d5dca2b",
     "grade": false,
     "grade_id": "cell-748463612fac3011",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pooch\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b5b53-2e59-4f16-a854-4d3984545265",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "808a98ccb946005b6a9d2d77263e223a",
     "grade": false,
     "grade_id": "cell-4c1f16890c9f6d89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Download data\n",
    "\n",
    "Now download the required data and helper files (same as previous notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e3c723-9fff-429c-a2d0-f7af793b01d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b7316b0b80fe0df0441e95180db4e91",
     "grade": false,
     "grade_id": "cell-c7e33380c498caf0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# download data\n",
    "DATA_REPO = \"https://raw.githubusercontent.com/uvapl/recommender-systems/main/data/m1/\"\n",
    "\n",
    "print(\"downloading data files\")\n",
    "for fname in [\"ratings_X_known.csv\", \"ratings_y_known.csv\", \"ratings_extended.csv\"]:\n",
    "    pooch.retrieve(url = DATA_REPO + fname, known_hash=None, fname=fname, path=\"data\", progressbar=True)\n",
    "for fname in [\"tests_m1.py\"]:\n",
    "    pooch.retrieve(url = DATA_REPO + fname, known_hash=None, fname=fname, path=\".\", progressbar=True)\n",
    "print(\"done!\")\n",
    "\n",
    "import tests_m1\n",
    "\n",
    "# read data\n",
    "X_known = pd.read_csv(\"data/ratings_X_known.csv\", index_col = \"userID\")\n",
    "y_known = pd.read_csv(\"data/ratings_y_known.csv\", index_col = \"userID\").iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8349def1-932a-4e73-8c18-d868fe143b30",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47b612059420e7506fce742a88e9f5dc",
     "grade": false,
     "grade_id": "cell-cb32a7044d63e47c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Train/Test Split\n",
    "\n",
    "To evaluate how well a recommender system performs, we need a way to measure how accurately it predicts user ratings. A common approach is to **split the data into a training set and a test set**. The idea is simple: for the test set, we pretend that we do not know the true ratings (even though we actually do). We then let the algorithm predict those ratings and later check how close the predictions were.\n",
    "\n",
    "We first learn user similarities *only* from the training set. Then we use these learned similarities to predict the ratings in the test set. This separation is crucial: the test set represents data that the model has not seen before, allowing us to evaluate how well the algorithm generalizes to new users or new rating situations.\n",
    "\n",
    "So we divide both:\n",
    "\n",
    "* the `X` data (the feature ratings for the movies we are **not** predicting), and\n",
    "* the `y` data (the **target** ratings we *do* want to predict)\n",
    "\n",
    "into two parts:\n",
    "\n",
    "* **Training set (`X_train`, `y_train`)** — the data the model is allowed to learn from.\n",
    "* **Test set (`X_test`, `y_test`)** — the data the model must predict without having seen it.\n",
    "\n",
    "After making predictions for the test set, we compare them to the actual ratings in `y_test`. This gives us an objective measure of model performance. We typically call the predicted ratings **`y_hat`** (written mathematically as $\\hat{y}$). So in the end, we want to evaluate **how close `y_hat` is to `y_test`**.\n",
    "\n",
    "In the next step, you will implement a simple function that creates such a train/test split for Pandas DataFrames and Series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0317c1-cb8b-4647-a4a9-05f149f03f90",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07811d2877601ffb46b4588e15b97121",
     "grade": false,
     "grade_id": "cell-e68e5191b171e32b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 1\n",
    "*2 pts.*\n",
    "\n",
    "Complete the function `train_test_split()` below. It takes the feature data `X`, the target data `y`, and a `test_size` argument that specifies which proportion of the data should be used for testing (for example, `test_size = 0.3` means that 30% of the data will be set aside as the test set).\n",
    "\n",
    "The function should return four objects:\n",
    "\n",
    "* **`X_train`** — the portion of `X` used for training\n",
    "* **`X_test`** — the portion of `X` used for testing\n",
    "* **`y_train`** — the corresponding training targets\n",
    "* **`y_test`** — the corresponding test targets\n",
    "\n",
    "Make sure that `X_train` aligns with `y_train`, and `X_test` aligns with `y_test`, and that the split preserves the original indices. And, make sure that the split is random (e.g., not the first 70% rows for training and the next 30% for testing). So, calling it multiple times should give different splits each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c1026-d90e-43c7-a70a-5c171ed12ccb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a0ca2b543b2eea670173a58a3f698c6",
     "grade": false,
     "grade_id": "cell-075bb961de7a94f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(X: pd.DataFrame, y: pd.Series, test_size: float = 0.3) -> (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series):\n",
    "    # your code here\n",
    "\n",
    "# apply to the provided data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_known, y_known)\n",
    "\n",
    "display(X_train)\n",
    "display(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df5502f-cab7-426d-93b9-9c29bdbb7600",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02c8a34f6de2dcc2382c79115322444a",
     "grade": true,
     "grade_id": "cell-7ed41e2a991cd10e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test your solution\n",
    "\n",
    "tests_m1.evaluation_01(train_test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff50939d-3933-41dc-8fa3-a2b8e2359bbb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c78fd618547ad38362fbf9a6093a916",
     "grade": false,
     "grade_id": "cell-9a2b97b84f157331",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Using scikit-learn for kNN Regression\n",
    "\n",
    "In the previous notebook, you implemented kNN regression **manually**, step by step. This helped you understand how cosine similarity, neighbor selection, and weighted averaging together produce a predicted rating. However, for real applications we typically rely on well-tested, optimized libraries rather than writing the algorithm from scratch.\n",
    "\n",
    "In this notebook, we switch to **scikit-learn**, a widely used machine learning library in Python. Scikit-learn provides a reliable and efficient implementation of kNN through the `KNeighborsRegressor` class. Its internal code is optimized, thoroughly tested, and handles many edge cases automatically.\n",
    "\n",
    "The workflow in scikit-learn always follows the same pattern:\n",
    "\n",
    "1. **Create a model object** (e.g., `KNeighborsRegressor(...)`).\n",
    "2. **Fit** the model on the data using `.fit(X_train, y_train)`.\n",
    "3. **Predict** on new data using `.predict(X_test)`.\n",
    "\n",
    "We will still use cosine similarity, but scikit-learn internally works with **cosine distance** (which is `1 - cosine_similarity`). To make the weighting consistent with our earlier implementation, we include a small conversion step that transforms distances back into similarities.\n",
    "\n",
    "The code below shows how to build a kNN regressor using scikit-learn and apply it to obtain predicted ratings for our unknown users. It produces the same results as the version you implemented in the previous notebook, just much, much faster.\n",
    "\n",
    "Note that, instead of using `X_known`, `y_known`, `X_unknown`, and `y_unknown` as in the previous notebook, we now use the terms `X_train`, `y_train`, `X_test`, and `y_test`. This terminology is more appropriate in the context of **evaluation**, as you have seen in the previous exercise.\n",
    "\n",
    "Load the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1b36e5-167c-437f-83e3-77141917bc24",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1388374e4e4c2f864bc4eb5e379bfd8e",
     "grade": false,
     "grade_id": "cell-070c33a47867a022",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "def knn_regression(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, k: int = 3) -> pd.Series:\n",
    "    # sklearn uses cosine distance, not similarity. \n",
    "    # We need to convert distance back to similarity so weighting \n",
    "    # is in line with the Aggarwal book.”  \n",
    "    def weights_conversion(dist):\n",
    "        # conversion by sklearn: dist = 1 - similarity = 1 - cos(a)\n",
    "        # undoing conversion for weights: similarity = 1 - dist = cos(a)\n",
    "        return np.clip(1 - dist, a_min=0, a_max=None)\n",
    "        \n",
    "    # setup sklearn knn regression object\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, metric=\"cosine\", weights=weights_conversion)\n",
    "\n",
    "    # add training data\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # predict for test data and convert result back to Pandas Series object\n",
    "    predicted_array = knn.predict(X_test)\n",
    "    predicted_series = pd.Series(predicted_array, index = X_test.index, name = y_train.name)\n",
    "    \n",
    "    return predicted_series\n",
    "\n",
    "# verify that it produces the same results as in the previous notebook\n",
    "print(knn_regression(X_train, y_train, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31eae5-1808-404a-a60f-1fd993234e4a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e606032bbc824cd97b0a101e2e7b9cc0",
     "grade": false,
     "grade_id": "cell-0bdeb9d434d65cc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Apply kNN to the actual data\n",
    "Now, let's run the kNN alogithm on the date you splitted in the previous assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498ad77-3ea5-46c3-bf53-8d33c9be99a6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a03577fcae5a4bdfe97f27163889c659",
     "grade": false,
     "grade_id": "cell-f597804f1abcc3d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_hat = knn_regression(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9faea10-9270-4d85-81a6-69a29b941f7b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c6125ec018ca8a4066a1e98ccb1653e",
     "grade": false,
     "grade_id": "cell-14013f7722454018",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Evaluating Regression: Mean Squared Error\n",
    "\n",
    "Now that we can make predictions, we need a way to measure **how good** those predictions are. We do this by computing the similarity between the predicted ratings (`y_hat`) and the test ratings (`y_test`). Now you might be temped to use cosine similarity for this, and that would actually not be a bad approach, but we will use a slightly different metric called the **mean squared error**.\n",
    "\n",
    "We first compute an **error** (deviation) for each prediction, then combine those errors into a single score.\n",
    "\n",
    "For each sample in the test set, we compute the deviation:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "<thead>\n",
    "<tr style=\"text-align: right;\">\n",
    "<th>user</th><th></th>\n",
    "<th>actual<br>rating</th><th>-</th>\n",
    "<th>predicted<br>rating</th><th>=</th>\n",
    "<th>difference</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>U168</td><td></td><td>-0.5</td><td>-</td><td>1.48</td><td>=</td><td>-1.98</td></tr>\n",
    "<tr><td>U169</td><td></td><td> 1.0</td><td>-</td><td>0.46</td><td>=</td><td> 0.54</td></tr>\n",
    "<tr><td>U170</td><td></td><td>-0.5</td><td>-</td><td>1.63</td><td>=</td><td>-2.13</td></tr>\n",
    "<tr><td>U171</td><td></td><td>-0.5</td><td>-</td><td>1.46</td><td>=</td><td>-1.96</td></tr>\n",
    "<tr><td>U172</td><td></td><td> 0.5</td><td>-</td><td>0.49</td><td>=</td><td> 0.01</td></tr>\n",
    "<tr><th colspan=\"8\" style=\"text-align:center;\">...</th></tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "Once we have all individual errors, we need a single number that summarizes how well the model performed.\n",
    "A widely used metric for this is the **mean squared error (MSE)**.\n",
    "\n",
    "We take the square of each error and then compute the average:\n",
    "\n",
    "$\n",
    "\\text{mean squared error} =\n",
    "\\frac{(-1.98)^2 + 0.54^2 + (-2.13)^2 + (-1.96)^2 + 0.01^2 + \\ldots}{N}\n",
    "$\n",
    "\n",
    "where (N) is the number of samples in the test set.\n",
    "\n",
    "The MSE is one of the most common evaluation metrics in data science and machine learning. You will encounter it frequently in later courses and real-world projects.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$\n",
    "\\text{mse} = \\frac{1}{N} \\sum_{i=1}^N (a_i - p_i)^2\n",
    "$\n",
    "\n",
    "where\n",
    "\n",
    "* $a_i$ is the **actual** rating,\n",
    "* $p_i$ is the **predicted** rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca8490b-c164-44fd-946a-d2861f9b6b4d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0593c98ec6d92434ef5e061556713305",
     "grade": false,
     "grade_id": "cell-a7f27463b1814ad3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 2\n",
    "*2 pt.*\n",
    "\n",
    "Implement the `mse()` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956fb6f4-825e-478f-9551-73e775fb12ec",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e99766af2feecb07760ef3bf34f05a4",
     "grade": false,
     "grade_id": "cell-fb2322f06e6e954e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mse(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "    # your code here\n",
    "\n",
    "mse(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ba315-c6ed-4dc3-a093-f56de87ef5f9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49dd46f24e64d5a6bbc792defc38231c",
     "grade": true,
     "grade_id": "cell-271d55da7aaa7720",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test your solution\n",
    "tests_m1.evaluation_02(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f04de78-72fc-4306-8ada-139ad8c5ba1e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c3080748b87a67192e4f941b79e34eb",
     "grade": false,
     "grade_id": "cell-220a499d460b900d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Baseline\n",
    "\n",
    "If everything is implemented correctly, the mean squared error for this dataset should be around **1.3**. But is that good?\n",
    "On its own, the value of the **mse** is hard to interpret. A number like 1.3 does not directly tell us whether the recommender system is “good” or “bad.”\n",
    "\n",
    "The real strength of mse lies in **comparison**. It allows us to evaluate which method performs better under the same conditions. For example, we can compare the mse of user-based filtering versus item-based filtering, or compare different similarity metrics, different values of (k), or different preprocessing steps.\n",
    "\n",
    "In other words, mse is most useful as a **relative** measure rather than an absolute one.\n",
    "\n",
    "A particularly important comparison is with simple **baseline models**—naive prediction strategies that do not use similarities or machine learning at all. One such baseline is making random predictions. A slightly better baseline is to predict the **mean rating of each movie**. Below, we will compare our recommender system to this latter baseline to see whether the kNN model actually provides an improvement.\n",
    "\n",
    "The function below does exactly this. It returns a predicted rating that is just the mean of the `y_train`. So the prediction is the exact same value for all users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dfe637-dda7-474f-bee2-751a071bfc18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9f0f09e36a955e22a8bdf8b747b5ca1",
     "grade": false,
     "grade_id": "cell-4721ab31c2c9c031",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def baseline_mean_prediction(y_train, X_test):\n",
    "    return pd.Series(y_train.mean(), index=X_test.index)\n",
    "\n",
    "y_hat_mean = baseline_mean_prediction(y_train, X_test)\n",
    "display(y_hat_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8d59b4-5ca4-4fcf-8471-48edcb95d5fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9f0806c586c3260a66df16e194e3ec5",
     "grade": false,
     "grade_id": "cell-a9b603194c631ae6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's compute the mean squared error for this baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5f4c8-483d-4935-b1ad-9c5a324f88e9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4676b6feef8d9844f3cfa1732ce8fece",
     "grade": false,
     "grade_id": "cell-a9d1e63e4355e776",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mse(y_test, y_hat_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160dd32e-b7be-4e40-9c16-05f2d8f67e13",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60f901c0790274228b66fcc2289affb0",
     "grade": false,
     "grade_id": "cell-823fb00781b48a8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Evaluating Classification: Precision and Recall\n",
    "\n",
    "We have seen that kNN performs much better than the baseline model. But does that automatically make it a *good* recommender system?\n",
    "\n",
    "Not necessarily. An important question is how meaningful the **mse** metric is for the final task. To answer that, we need to think about how the recommender system is actually used. What does the system *do* with the predicted ratings?\n",
    "\n",
    "* Does it show the user a top-10 list of items with the highest predicted scores?\n",
    "* Does it recommend all movies whose predicted rating is above a certain threshold?\n",
    "* Does the user see randomly selected recommendations drawn from a pool of “good” items?\n",
    "\n",
    "Depending on how the recommendations are used, mse may or may not be the right evaluation measure.\n",
    "\n",
    "## Recommended vs. Hidden Items\n",
    "\n",
    "To make evaluation concrete, we will assume the following workflow for the recommender system:\n",
    "\n",
    "1. **Recommended items:**\n",
    "   For each user, we mark items with a predicted rating **greater than or equal to a threshold** (e.g., 0.25) as items that *should be recommended*.\n",
    "\n",
    "2. **Hidden items:**\n",
    "   Items with predicted ratings **below the threshold** are marked as items that should *not* be recommended.\n",
    "\n",
    "3. **Recommendation display:**\n",
    "   The system then selects (N) random items from the recommended pool to show to the user.\n",
    "   (We won’t elaborate further on this step, but for now it’s important to know that *every recommended item has an equal chance of being shown*.)\n",
    "\n",
    "> With this setup, our recommender system becomes a **classification problem**:\n",
    "> For each (user, movie) pair, we must decide whether it belongs to the *recommended* class or the *hidden* class.\n",
    "> This type of binary classification task is very common in machine learning.\n",
    "\n",
    "This shift from regression (predicting a rating) to classification (predict/not predict) means that mse is no longer the most meaningful performance metric. Instead, we will evaluate the system using **precision** and **recall**, which better capture how well a classifier behaves.\n",
    "\n",
    "## Classify\n",
    "\n",
    "We now move from predicting ratings to **classifying** items as *recommended* or *not recommended*.\n",
    "Use the `recommend()` function below (which is similar to the one you implemented in the previous notebook) to produce these classifications for both the kNN model and the baseline model.\n",
    "\n",
    "The function returns a Pandas Series containing `True` and `False` values:\n",
    "\n",
    "* `True` means the predicted rating is above the threshold (0.25) → **recommend**\n",
    "* `False` means it is below the threshold → **do not recommend**\n",
    "\n",
    "This function is applied to:\n",
    "\n",
    "* the kNN predictions (`recommendations_knn`)\n",
    "* the baseline predictions (`recommendations_mean`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0cb5c1-b83a-43a0-a083-e1b5ab78d7d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "305a3c9f3583cb909c2458a08c9e4ba3",
     "grade": false,
     "grade_id": "cell-b530eefcad8601f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def recommend(predictions, threshold):\n",
    "    return predictions >= threshold\n",
    "\n",
    "threshold = 0.25\n",
    "recommendations_knn = recommend(y_hat, threshold)\n",
    "print(recommendations_knn)\n",
    "recommendations_mean = recommend(y_hat_mean, threshold)\n",
    "print(recommendations_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f7eb7f-cd9c-438e-a8d9-d12121c73165",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c1b94c7a9d6eff2357cdfd32c89aeaa",
     "grade": false,
     "grade_id": "cell-94e415e3f53dde88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test Data (liked)\n",
    "To evaluate the performance of the recommender system, we need to know whether the users in our test set (`y_test`) **actually liked** the movies. We *could* infer this from their ratings, but that is not necessary. This information is already available in our dataset: the file **`ratings_extended.csv`** contains a column indicating whether a user liked a movie.\n",
    "\n",
    "All we need to do is extract the relevant entries.\n",
    "The cell below reads `ratings_extended.csv`, selects the rows corresponding to the user–movie pairs in `y_test`, and creates a new Series called `liked_test` that tells us, for each test sample, whether the user liked the movie in reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869bde00-a3f9-4d91-bc5f-ffa0da3f9c7c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f39df571f222b42f38c893ebe17a010",
     "grade": false,
     "grade_id": "cell-2a8e884d8a8276a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"data/ratings_extended.csv\", dtype = {\"liked\": bool})\n",
    "liked = ratings.pivot(index = 'userID', columns = 'movieID', values = 'liked')[\"M4096\"]\n",
    "liked_test = liked.loc[y_test.index].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcba6501-b9b4-4a9d-a307-1c74ebb916c0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46076e96ddef44b8fe93425cc816f99f",
     "grade": false,
     "grade_id": "cell-71d891997dec7c88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Confusion\n",
    "\n",
    "We now have all the information needed to evaluate the recommender system’s performance:\n",
    "\n",
    "* the kNN-based recommendations (`recommendations_knn`)\n",
    "* the baseline recommendations (`recommendations_mean`)\n",
    "* the actual like/dislike information (`liked_test`)\n",
    "\n",
    "Some terminology:\n",
    "In evaluation, the **actual** data is often described as *used* (liked) and *unused* (not liked).\n",
    "The **predicted** data is described as *recommended* (the system would show it) and *hidden* (the system would not show it).\n",
    "\n",
    "We begin by counting how many items were **correctly recommended**—items the system recommended and the user actually liked. These are the **true positives**.\n",
    "\n",
    "Given our four categories (recommended, hidden, used, unused), we can define the standard classification outcomes:\n",
    "\n",
    "* **True positives (TP):** recommended *and* liked\n",
    "* **False positives (FP):** recommended *but not* liked\n",
    "* **True negatives (TN):** hidden *and not* liked\n",
    "* **False negatives (FN):** hidden *but actually* liked\n",
    "\n",
    "These four values are typically arranged in a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), a standard evaluation tool:\n",
    "\n",
    "|             | Liked (Used) | Not Liked (Unused) |\n",
    "| ----------- | ------------ | ------------------ |\n",
    "| Recommended | TP           | FP                 |\n",
    "| Hidden      | FN           | TN                 |\n",
    "\n",
    "### Question 3\n",
    "\n",
    "*3 pts.*\n",
    "\n",
    "Implement the `confusion()` function below. It should take the predicted recommendations and the actual data as input and return a **2×2 DataFrame** containing the confusion matrix in the format shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49e033-4dd0-4a93-860b-0f1fc3780a0d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3b16e2fb7fc41d39156fce99fb2b718",
     "grade": false,
     "grade_id": "cell-3d9200556b1e1b97",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def confusion(y_true: pd.Series, y_pred: pd.Series) -> pd.DataFrame:\n",
    "    # your code here\n",
    "\n",
    "confusion_knn = confusion(liked_test, recommendations_knn)\n",
    "print(confusion_knn)\n",
    "confusion_mean = confusion(liked_test, recommendations_mean)\n",
    "print(confusion_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c10509-30aa-4686-ae0c-a1d955708f1e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a2a63e1177c00f4415ef51bfacdb4e4",
     "grade": true,
     "grade_id": "cell-922be7ed89b8a305",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test your solution\n",
    "\n",
    "tests_m1.evaluation_03(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baef0ba0-08bd-457c-bfcd-e4f96bbe85ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15a6ca4ce166d279df92a70cf7f3c259",
     "grade": false,
     "grade_id": "cell-bdda59cccc1db20b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Precision\n",
    "\n",
    "A commonly used evaluation metric in classification tasks is **precision**. Precision answers the question:\n",
    "\n",
    "**When the system recommends a movie, how often is that recommendation actually relevant?**\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$\n",
    "\\textrm{precision} = \\frac{\\textrm{true positives}}{\\textrm{\\# recommended items}} = \\frac{\\textrm{true positives}}{\\textrm{true positives + false positives}}\n",
    "$$\n",
    "\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "* If **all** recommended movies are relevant\n",
    "  (i.e., $\\text{false positives} = 0$) ->\n",
    "  $\\text{precision} = 1$\n",
    "\n",
    "* If **none** of the recommended movies are relevant\n",
    "  (i.e., $\\text{true positives} = 0$) -> $\\text{precision} = 0$\n",
    "\n",
    "Precision is particularly useful when we care about the *quality* of recommendations rather than the quantity. For more information, see: [https://en.wikipedia.org/wiki/Precision_and_recall](https://en.wikipedia.org/wiki/Precision_and_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b1a57-d2e5-4b4b-b884-c7ac478f626e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7293e16c1fb7785b919fc7ed528c14f3",
     "grade": false,
     "grade_id": "cell-118fb6b3e929ad83",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 4\n",
    "*2 pts.*\n",
    "\n",
    "Implement the function `precision()` below. It should take the predicted recommendations and the actual like/dislike data as input, and return the **precision** of the recommender system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24f4ea-603e-4abb-a6ae-d5ecf0bb2d4b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e99cd08e31ab893c40a2cb66fb25ab23",
     "grade": false,
     "grade_id": "cell-489f80dd18812c20",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def precision(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "    # your code here\n",
    "\n",
    "precision_knn = precision(liked_test, recommendations_knn)\n",
    "print(precision_knn)\n",
    "precision_mean = precision(liked_test, recommendations_mean)\n",
    "print(precision_mean)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b05211-73a6-482e-a4b2-d835a8d5ce43",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d4845df3972b6492f412595e4c47b8e",
     "grade": true,
     "grade_id": "cell-cfa6a69ad33cd73d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test your solution\n",
    "\n",
    "tests_m1.evaluation_04(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18b911b-a9fa-440c-bdca-20589f464f4f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "465ade60a474d22b5659e4d028118286",
     "grade": false,
     "grade_id": "cell-ece13b0f924d723a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Recall\n",
    "\n",
    "Precision gives us valuable insight into how well the algorithm performs, but it does not tell the whole story. Many other metrics are important when evaluating recommender systems. One of the most common metrics used alongside precision is **recall**.\n",
    "\n",
    "Recall answers a different question:\n",
    "\n",
    "**Of all the items a user would actually like, how many does the algorithm successfully recommend?**\n",
    "\n",
    "In other words: if a user would enjoy a movie, does the system manage to recommend it?\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$\n",
    "\\textrm{recall} = \\frac{\\textrm{true positives}}{\\textrm{\\#used items}} = \\frac{\\textrm{true positives}}{\\textrm{true positives + false negatives}}\n",
    "$$\n",
    "\n",
    "\n",
    "If there are **no false negatives** (that is, if the algorithm recommends *every* movie the user would have liked) then:\n",
    "\n",
    "$$\n",
    "\\text{recall} = 1.\n",
    "$$\n",
    "\n",
    "A low recall indicates that the recommender is missing many potentially good recommendations, even if the ones it does make are accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bcb831-1b0f-4602-b752-3f8498946de2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c6f947a3733169db725a4b14f8352ccd",
     "grade": false,
     "grade_id": "cell-4d5b3506b9450664",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 5\n",
    "*2 pts.*\n",
    "\n",
    "Implement the function `recall()` below. It should take the predicted recommendations and the actual like/dislike data as input, and return the **recall** of the recommender system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e047c4-71ce-41c7-9731-8f61742ea04f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a16db40078e879509fc6f1167fa57d43",
     "grade": false,
     "grade_id": "cell-19290ddcab62e19e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def recall(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "    # your code here\n",
    "\n",
    "recall_knn = recall(liked_test, recommendations_knn)\n",
    "print(recall_knn)\n",
    "recall_mean = recall(liked_test, recommendations_mean)\n",
    "print(recall_mean)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a3350-de56-4ec9-8f85-39a9ec552842",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "533066c89a8d90d190116593ef22adef",
     "grade": true,
     "grade_id": "cell-b6a93cd5d74db60f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test your solution\n",
    "tests_m1.evaluation_05(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab120e-3a00-4b3f-a079-65c608d027b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1a22d5b672d2c7d07588ae5253f2998",
     "grade": false,
     "grade_id": "cell-aa7f67509faa63b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Remember that the dataset we used in this notebook is very small. This means we cannot reliably judge how well the algorithm would perform in real-world scenarios. The purpose here is to practice the evaluation methods, not to draw conclusions about actual performance.\n",
    "\n",
    "But what we can say is this: Precision and recall often behave in opposing ways. Improving one can easily worsen the other. For example, increasing the recommendation threshold or choosing a smaller (k) might raise **precision** (because only very confident recommendations are shown) but this usually lowers **recall**, since fewer potentially relevant items are recommended. Lowering the threshold or increasing (k) tends to have the opposite effect: **recall** improves, but precision may drop.\n",
    "\n",
    "You can try this effect for yourself by playing around with the threshold and the k-value.\n",
    "\n",
    "The challenge in designing a recommender system is to find the right **balance** between these two metrics. In the next notebook, we will use a much larger dataset to explore how these measures relate and how different choices in the algorithm (such as the threshold) influence this trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4584b4-e57e-44e8-b33f-e80f27530bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
